<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Android Camera API | The Essence of Software</title>
<meta name="keywords" content="">
<meta name="description" content="Applying concept design to the camera rotation API">
<meta name="author" content="Daniel Jackson">
<link rel="canonical" href="https://essenceofsoftware.com/studies/larger/rotation/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.2e7233f1eda5b0ed5b65d97b4091e50cb7c049a0f885a948abb89369b8a860cc.css" integrity="sha256-LnIz8e2lsO1bZdl7QJHlDLfASaD4halIq7iTabioYMw=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://essenceofsoftware.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://essenceofsoftware.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://essenceofsoftware.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://essenceofsoftware.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://essenceofsoftware.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-WRQ6HWW3ZW"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WRQ6HWW3ZW', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Android Camera API" />
<meta property="og:description" content="Applying concept design to the camera rotation API" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://essenceofsoftware.com/studies/larger/rotation/" /><meta property="og:image" content="https://essenceofsoftware.com/eos-twitter-card.png"/><meta property="article:section" content="studies" />
<meta property="article:published_time" content="2023-07-11T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-07-11T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://essenceofsoftware.com/eos-twitter-card.png"/>

<meta name="twitter:title" content="Android Camera API"/>
<meta name="twitter:description" content="Applying concept design to the camera rotation API"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Case Studies",
      "item": "https://essenceofsoftware.com/studies/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Longer case studies",
      "item": "https://essenceofsoftware.com/studies/larger/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Android Camera API",
      "item": "https://essenceofsoftware.com/studies/larger/rotation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Android Camera API",
  "name": "Android Camera API",
  "description": "Applying concept design to the camera rotation API",
  "keywords": [
    
  ],
  "articleBody": "API Design at Google Concept design was developed for shaping the functionality of apps, and because it emphasizes the underlying semantics behind the user interface, it should be straightforward to apply it to the design of services. But can it be applied to APIs?\nRecently, inspired by discussions with Meital Tagor Sbero, a UX research manager at Google, I’ve started looking into whether it might be useful in the design of APIs.\nMeital runs a team working on making APIs easier for programmers to use. They have been applying cognitive dimensions of notations, a collection of heuristic principles for usability (due to Thomas Green and Marian Petre) to the design of Android APIs.\nOne of the heuristics is called closeness of mapping and suggests that a notation (or in this case an API) should correspond closely to the problem world. Concept design seems to offer a way to pursue this goal.\nAs an example of an API that programmers find hard to understand, Meital pointed me to the Android Camera API. I decided to focus for a small case study on the Rotation API, because it’s small and seems to have some complexities that concepts can address. I’ve developed a concept model of this API which I believe resolves some of the problems in understanding it, but the model is not complete and it may well contain errors that reflect my own misunderstandings. I’ve also not done the hard work of mapping the concept actions and states to the programmatic components of the API.\nWhy the Rotation API is tricky Before showing you the concepts I came up with, I need to explain why the Rotation API isn’t so easy to understand. I’ll do this by pointing to explanations within the official API documentation.\nThe documentation starts by introducing some key terms:\nReading this, I found myself confused:\nTarget = display? If the display rotation “represents the degrees by which the device is rotated counter-clockwise from its natural orientation” and the target rotation “represents the number of degrees through which to rotate the device clockwise to reach its natural orientation”, why aren’t the two always equal? And indeed, in the graphical examples that follow, the two are equal.\nHow many orientations? The display orientation is said to have only four values, but the phone can surely be held in between these. Indeed, the code sample that appears later shows an onOrientationChanged listener that gives an orientation in degrees:\nDisplay or device? The definitions seem to use the terms “display” and “device” interchangeably, but later it will transpire that the display orientation may not match the device orientation: Furthermore, a second code sample shows an alternative way to set the target rotation not using the onOrientationChanged listener but using the onDisplayChanged listener instead:\nThe next section introduces two new terms: sensor orientation, “a constant value, which represents the degrees (0, 90, 180, 270) the sensor is rotated from the top of the device when the device is in a natural position” and the image rotation which “describes how the data should be rotated clockwise to appear upright.” A series of diagrams then illustrate sensor orientations for different phones:\nThis puzzled me. How can the sensor be rotated and not be aligned with the phone? If the Pixel 3XL indeed has a sensor that is rotated 90 degrees, that would mean that the sensor would be in landscape mode when the phone is held upright in portrait mode.\nA Conceptual Take Here is my attempt to resolve these issues and explain what I believe is going on here.\nFirst, the phone contains an accelerometer that gives its rotation from the upright position. I’ll model this in a simple concept with just one state component (holding the current rotation) and an action that is called whenever the rotation changes:\nconcept DeviceRotation purpose provide physical rotation info from accelerometer principle when user rotates device R degrees counterclockwise from upright position, rotationUpdated(R) occurs state\u2028rotation: Degrees actions // system action performed whenever // change in rotation is detected // outputs current rotation and sets rotation = r rotationUpdated (out r: Degrees) I’m using the word “rotation” throughout for an angular measurement, and reserving the word “orientation” for portrait, landscape, etc. But note that this rotationUpdated action is the onOrientationChanged listener in the API.\nNote that the notion of physical rotation is non-trivial, and isn’t even defined when the phone is parallel to the ground: you’ll have noticed this when you’ve taken photos of something on the ground and been surprised that the resulting image’s orientation seems wrong. But the notion should be intuitively clear enough that we can proceed to the other concepts.\nOur second concept models just one aspect of a digital camera, namely how a captured image has a rotation associated with it:\nconcept Camera purpose take images with recorded rotations principle after setAdditionalRotation (r); capture (i) {i.rotation = r + defaultRotation} state const defaultRotation: Degrees // defined by hardware additionalRotation: Degrees rotation: Image -\u003e Degrees rows: Image -\u003e seq seq Pixel actions setAdditionalRotation (r: Degrees)\t// make new image i with i.rotation = defaultRotation + additionalRotation capture (out i: Image) Images are represented here naively as rows of pixels. Each image has a rotation associated with it. This rotation is intended to be applied to the image when it is displayed, and is chosen so that the image will have the appropriate orientation.\nFor example, if the camera is positioned to take an image in portrait orientation, but the first row in the recorded image corresponds to the pixels in the rightmost column as seen through the camera viewfinder, the image will need to be rotated by 90 degrees (clockwise) to be displayed correctly.\nThis scenario might still occur if the portrait orientation is the natural orientation of the camera (as it is for most mobile phones), because it’s not necessary to require the sensor always record an image so that rows are horizontal in the natural orientation.\nThis is why there is a hardware-specific defaultRotation that holds the rotation that must be applied when the camera is in its natural orientation. So for a camera that has a natural portrait orientation, but whose sensor records rows as just described, the defaultRotation will be 90, and that will determine the image rotation. Now if the camera is held in landscape orientation (by being rotated 90 degrees counterclockwise), the additionalRotation will be set to -90 degrees. As a result, the rotation of the image will be the sum of these, and will be zero, reflecting the fact that in this position the sensor happens to record the image in the expected orientation.\nThe defaultRotation is what was called “sensor rotation” in the API documentation, and the rotation associated with the captured image is the “image rotation”.\nNow finally the most subtle concept. As the user rotates the camera, we need to infer from the extent of the rotation whether the intended orientation is portrait or landscape. Clearly when the camera is perfectly upright, a portrait orientation is intended; and when it is perfectly on its side, rotated 90 degrees counterclockwise, a landscape orientation is intended.\nBut what about in between? Here’s what you might see as you rotate a camera from portrait through to landscape:\nNear the extremes, the intended orientation is clear, but in the middle (as illustrated by the example shown on the bottom right) it seems arbitrary. Perhaps the intended orientation could be inferred by looking at the sensor image, but the angle of rotation alone won’t answer the question definitively.\nTo model this, we define a concept that infers an orientation from a given angle:\nconcept UserOrientation purpose infer intended camera orientation from device angle principle (1) after updateDeviceAngle (a); getInferredOrientation (o, d) {o is the inferred orientation and d its value in degrees} (2) after toggleLock(); updateDeviceAngle (a); getInferredOrientation (o, d) {o is PORTRAIT and d is 0} state Orientation = {PORTRAIT, LANDSCAPE, PORTRAIT_REV, LANDSCAPE_REV} inferredOrientation: Orientation deviceAngle: Degrees orientationLock: Bool = false action toggleLock () updateDeviceAngle (angle: Degrees) getInferredOrientation (out orientation: Orientation, inDegrees: Degrees) The concept holds in its state:\nthe inferred orientation of the device, which has one of four values (including the “reverse” versions of portrait and landscape, namely when the camera is upside down); the angle at which the device is being held; and an orientation lock field, which when true holds the inferred orientation to be the standard portrait orientation. There are three actions:\ntoggleLock simply toggles the orientation lock; updateDeviceAngle updates the angle; getInferredOrientation returns as outputs the inferred orientation, and its equivalent in degrees. This last action embodies whatever (arbitrary) decision we make on when to switch between orientations.\nSynchronizing the concepts Now we can put the concepts together:\napp Camera include concept DeviceRotation concept Camera concept UserOrientation and connect their behaviors with synchronization. The essential one occurs when a device rotation is detected:\nsync updateRotation (out r, o: 0..359) when DeviceRotation.rotationUpdated (r) UserOrientation.updateDeviceAngle (r) UserOrientation.getInferredOrientation (_, o) Camera.setAdditionalRotation (o) When the accelerometer reports a change in the rotation of the device, a new inferred orientation is obtained from the new angle, and the additional rotation in the camera is updated accordingly.\nConnecting back to the API Now looking back at the API code sample we can understand it more readily:\nThe firing of the onOrientationChanged listener corresponds to the DeviceRotation.rotationUpdated action that fires the sync; the when statement that assigns rotation based on orientation is an implementation of the UserOrientation.getInferredOrientation action; and the setting of imageCapture.targetRotation is the Camera.setAdditionalRotation action.\nIn the alternative code sample,\nmy guess is that the following is going on. There is a separate and fuller implementation of the UserOrientation concept that rotates the display for apps in general (and is appropriately sync’d with the other concepts as above). The camera can use the orientation reported directly by this module, obtaining the rotation field of the display object. This presumably is affected by whether the orientation lock is on, and perhaps also by whether the app in question is using this feature.\nSummary In summary, we’ve resolved our initial questions as follows:\nThe word “orientation” seems to be used in two distinct ways: for the device rotation, and for the inferred orientation. The “sensor rotation” is just the default rotation associated with captured images, and the “image rotation” is their total associated rotation. Because the phone already includes a module that infers orientation and rotates the display accordingly, display orientation and device orientation are distinct, and a programmer might be able to obtain the display orientation directly, bypassing the need to make the inference in application code. What are the benefits of the conceptual view here? I’d argue that they are:\nA clean separation of concerns, into (a) obtaining the device rotation from the accelerometer; (b) storing rotations with images; and (c) inferring orientations from rotations. A purpose and simple scenarios (OPs) for the concepts, in particular the UserOrientation concept which the API docs didn’t make clear is heuristic. It remains to be seen how this kind of concept design can best be worked into API documentation. It’s clearly not ideal to have a textual notation for concepts that is different from the typical interface specs of APIs. One possibility is to represent concepts diagrammatically.\nAnother challenge involves mapping the concepts to the sometimes complex object-oriented patterns that are used in APIs, which are often more like frameworks (with application code written as subclassing extensions) than service interfaces.\n",
  "wordCount" : "1912",
  "inLanguage": "en",
  "datePublished": "2023-07-11T00:00:00Z",
  "dateModified": "2023-07-11T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Daniel Jackson"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://essenceofsoftware.com/studies/larger/rotation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "The Essence of Software",
    "logo": {
      "@type": "ImageObject",
      "url": "https://essenceofsoftware.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://essenceofsoftware.com" accesskey="h" title="The Essence of Software (Alt + H)">The Essence of Software</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://essenceofsoftware.com/ask/" title="FAQ">
                    <span>FAQ</span>
                </a>
            </li>
            <li>
                <a href="https://essenceofsoftware.com/buy/" title="Buy">
                    <span>Buy</span>
                </a>
            </li>
            <li>
                <a href="https://essenceofsoftware.com/reviews/" title="Reviews">
                    <span>Reviews</span>
                </a>
            </li>
            <li>
                <a href="https://essenceofsoftware.com/author/" title="Author">
                    <span>Author</span>
                </a>
            </li>
            <li>
                <a href="https://essenceofsoftware.com/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://essenceofsoftware.com/tutorials/" title="Tutorials">
                    <span>Tutorials</span>
                </a>
            </li>
            <li>
                <a href="https://essenceofsoftware.com/studies/" title="Case Studies">
                    <span>Case Studies</span>
                </a>
            </li>
            <li>
                <a href="https://essenceofsoftware.com/subscribe/" title="Subscribe">
                    <span>Subscribe</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Android Camera API
    </h1>
    <div class="post-description">
      Applying concept design to the camera rotation API
    </div>
    <div class="post-meta"><span title='2023-07-11 00:00:00 +0000 UTC'>July 11, 2023</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Daniel Jackson&nbsp;|&nbsp;<a href="https://forum.softwareconcepts.io" rel="noopener noreferrer" target="_blank">Comments</a>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#api-design-at-google" aria-label="API Design at Google">API Design at Google</a></li>
                <li>
                    <a href="#why-the-rotation-api-is-tricky" aria-label="Why the Rotation API is tricky">Why the Rotation API is tricky</a></li>
                <li>
                    <a href="#a-conceptual-take" aria-label="A Conceptual Take">A Conceptual Take</a></li>
                <li>
                    <a href="#synchronizing-the-concepts" aria-label="Synchronizing the concepts">Synchronizing the concepts</a></li>
                <li>
                    <a href="#connecting-back-to-the-api" aria-label="Connecting back to the API">Connecting back to the API</a></li>
                <li>
                    <a href="#summary" aria-label="Summary">Summary</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="api-design-at-google">API Design at Google<a hidden class="anchor" aria-hidden="true" href="#api-design-at-google">#</a></h2>
<p>Concept design was developed for shaping the functionality of apps, and  because it emphasizes the underlying semantics behind the user interface, it should be straightforward to apply it to the design of services. But can it be applied to APIs?</p>
<p>Recently, inspired by discussions with Meital Tagor Sbero, a UX research manager at Google, I’ve started looking into whether it might be useful in the design of APIs.</p>
<p>Meital runs a team working on making APIs easier for programmers to use. They have been applying <a href="https://en.wikipedia.org/wiki/Cognitive_dimensions_of_notations">cognitive dimensions of notations</a>, a  collection of heuristic principles for  usability (due to Thomas Green and Marian Petre) to the design of Android APIs.</p>
<p>One of the heuristics is called <em>closeness of mapping</em> and suggests that  a notation (or in this case an API) should correspond closely to the problem world. Concept design seems to offer a way to pursue this goal.</p>
<p>As an example of an API that programmers find hard to understand, Meital pointed me to the <a href="https://developer.android.com/training/camerax">Android Camera API</a>. I decided to focus for a small case study on the <a href="https://developer.android.com/training/camerax/orientation-rotation">Rotation API</a>, because it’s small and seems to have some complexities that concepts can address. I’ve developed a concept model of this API which I believe resolves some of the problems in understanding it, but the model is not complete and it may well contain errors that reflect my own misunderstandings. I’ve also not done the hard work of mapping the concept actions and states to the programmatic components of the API.</p>
<h2 id="why-the-rotation-api-is-tricky">Why the Rotation API is tricky<a hidden class="anchor" aria-hidden="true" href="#why-the-rotation-api-is-tricky">#</a></h2>
<p>Before showing you the concepts I came up with, I need to explain why the Rotation API isn’t so easy to understand. I’ll do this by pointing to explanations within the official API documentation.</p>
<p>The documentation starts by introducing some key terms:</p>
<p><img loading="lazy" src="rotation-terms.png" alt=""  />
</p>
<p>Reading this, I found myself confused:</p>
<ul>
<li>
<p><strong>Target = display?</strong> If the display rotation “represents the degrees by which the device is rotated counter-clockwise from its natural orientation” and the target rotation “represents the number of degrees through which to rotate the device clockwise to reach its natural orientation”, why aren’t the two always equal? And indeed, in the graphical examples that follow, the two are equal.</p>
</li>
<li>
<p><strong>How many orientations?</strong> The display orientation is said to have only four values, but the phone can surely be held in between these. Indeed, the code sample that appears later shows an <em>onOrientationChanged</em> listener that gives an orientation in degrees:</p>
</li>
</ul>
<p><img loading="lazy" src="orientation-changed.png" alt=""  />
</p>
<ul>
<li><strong>Display or device?</strong> The definitions seem to use the terms “display” and “device” interchangeably, but later it will transpire that the display orientation may not match the device orientation:</li>
</ul>
<p><img loading="lazy" src="display-device.png" alt=""  />
</p>
<p>Furthermore, a second code sample shows an alternative way to set the target rotation not using the <em>onOrientationChanged</em> listener but using the <em>onDisplayChanged</em> listener instead:</p>
<p><img loading="lazy" src="display-listener.png" alt=""  />
</p>
<p>The next section introduces two new terms: <em>sensor orientation</em>, “a constant value, which represents the degrees (0, 90, 180, 270) the sensor is rotated from the top of the device when the device is in a natural position” and the <em>image rotation</em> which “describes how the data should be rotated clockwise to appear upright.” A series of diagrams then illustrate sensor orientations for different phones:</p>
<p><img loading="lazy" src="image-rotation.png" alt=""  />
</p>
<p>This puzzled me. How can the sensor be rotated and not be aligned with the phone? If the Pixel 3XL indeed has a sensor that is rotated 90 degrees, that would mean that the sensor would be in landscape mode when the phone is held upright in portrait mode.</p>
<h2 id="a-conceptual-take">A Conceptual Take<a hidden class="anchor" aria-hidden="true" href="#a-conceptual-take">#</a></h2>
<p>Here is my attempt to resolve these issues and explain what I believe is going on here.</p>
<p>First, the phone contains an accelerometer that gives its rotation from the upright position. I’ll model this in a simple concept with just one state component (holding the current rotation) and an action that is called whenever the rotation changes:</p>
<pre><code>concept DeviceRotation
purpose provide physical rotation info from accelerometer
principle
  when user rotates device R degrees
  counterclockwise from upright position, rotationUpdated(R) occurs
state   rotation: Degrees
actions
  // system action performed whenever
  // change in rotation is detected
  // outputs current rotation and sets rotation = r
  rotationUpdated (out r: Degrees)
</code></pre>
<p>I’m using the word “rotation” throughout for an angular measurement, and reserving the word “orientation” for portrait, landscape, etc. But note that this <em>rotationUpdated</em> action is the <em>onOrientationChanged</em> listener in the API.</p>
<p>Note that the notion of physical rotation is non-trivial, and isn’t even defined when the phone is parallel to the ground: you’ll have noticed this when you’ve taken photos of something on the ground and been surprised that the resulting image’s orientation seems wrong. But the notion should be intuitively clear enough that we can proceed to the other concepts.</p>
<p>Our second concept models just one aspect of a digital camera, namely how a captured image has a rotation associated with it:</p>
<pre><code>concept Camera
purpose take images with recorded rotations
principle
  after setAdditionalRotation (r); capture (i)
    {i.rotation = r + defaultRotation}
state
  const defaultRotation: Degrees // defined by hardware
  additionalRotation: Degrees
  rotation: Image -&gt; Degrees
  rows: Image -&gt; seq seq Pixel
actions
  setAdditionalRotation (r: Degrees)	  
  // make new image i with i.rotation = defaultRotation + additionalRotation
  capture (out i: Image)
</code></pre>
<p>Images are represented here naively as rows of pixels. Each image has a rotation associated with it. This rotation is intended to be applied to the image when it is displayed, and is chosen so that the image will have the appropriate orientation.</p>
<p>For example, if the camera is positioned to take an image in portrait orientation, but the first row in the recorded image corresponds to the pixels in the rightmost column as seen through the camera viewfinder, the image will need to be rotated by 90 degrees (clockwise) to be displayed correctly.</p>
<p>This scenario might still occur if the portrait orientation is the natural orientation of the camera (as it is for most mobile phones), because it’s not necessary to require the sensor always record an image so that rows are horizontal in the natural orientation.</p>
<p>This is why there is a hardware-specific <em>defaultRotation</em> that holds the rotation that must be applied when the camera is in its natural orientation. So for a camera that has a natural portrait orientation, but whose sensor records rows as just described, the <em>defaultRotation</em> will be 90, and that will determine the image <em>rotation</em>. Now if the camera is held in landscape orientation (by being rotated 90 degrees counterclockwise), the <em>additionalRotation</em> will be set to -90 degrees. As a result, the <em>rotation</em> of the image will be the sum of these, and will be zero, reflecting the fact that in this position the sensor happens to record the image in the expected orientation.</p>
<p>The <em>defaultRotation</em> is what was called “sensor rotation” in the API documentation, and the <em>rotation</em> associated with the captured image is the “image rotation”.</p>
<p>Now finally the most subtle concept. As the user rotates the camera, we need to infer from the extent of the rotation whether the intended orientation is portrait or landscape. Clearly when the camera is perfectly upright, a portrait orientation is intended; and when it is perfectly on its side, rotated 90 degrees counterclockwise, a landscape orientation is intended.</p>
<p>But what about in between? Here’s what you might see as you rotate a camera from portrait through to landscape:</p>
<p><img loading="lazy" src="orientations.png" alt=""  />
</p>
<p>Near the extremes, the intended orientation is clear, but in the middle (as illustrated by the example shown on the bottom right) it seems arbitrary. Perhaps the intended orientation could be inferred by looking at the sensor image, but the angle of rotation alone won’t answer the question definitively.</p>
<p>To model this, we define a concept that infers an orientation from a given angle:</p>
<pre><code>concept UserOrientation
purpose
  infer intended camera orientation from device angle
principle
  (1) after updateDeviceAngle (a); getInferredOrientation (o, d)
    {o is the inferred orientation and d its value in degrees}
  (2) after toggleLock(); updateDeviceAngle (a); getInferredOrientation (o, d)
    {o is PORTRAIT and d is 0}
state
  Orientation = {PORTRAIT, LANDSCAPE, PORTRAIT_REV, LANDSCAPE_REV}
  inferredOrientation: Orientation
  deviceAngle: Degrees
  orientationLock: Bool = false
action
  toggleLock ()
  updateDeviceAngle (angle: Degrees)
  getInferredOrientation (out orientation: Orientation, inDegrees: Degrees)
</code></pre>
<p>The concept holds in its state:</p>
<ul>
<li>the inferred orientation of the device, which has one of four values (including the “reverse” versions of portrait and landscape, namely when the camera is upside down);</li>
<li>the angle at which the device is being held;</li>
<li>and an orientation lock field, which when true holds the inferred orientation to be the standard portrait orientation.</li>
</ul>
<p>There are three actions:</p>
<ul>
<li><em>toggleLock</em> simply toggles the orientation lock;</li>
<li><em>updateDeviceAngle</em> updates the angle;</li>
<li><em>getInferredOrientation</em> returns as outputs the inferred orientation, and its equivalent in degrees.</li>
</ul>
<p>This last action embodies whatever (arbitrary) decision we make on when to switch between orientations.</p>
<h2 id="synchronizing-the-concepts">Synchronizing the concepts<a hidden class="anchor" aria-hidden="true" href="#synchronizing-the-concepts">#</a></h2>
<p>Now we can put the concepts together:</p>
<pre><code>app Camera
include
  concept DeviceRotation
  concept Camera
  concept UserOrientation
</code></pre>
<p>and connect their behaviors with synchronization. The essential one occurs when a device rotation is detected:</p>
<pre><code>sync updateRotation (out r, o: 0..359)
  when DeviceRotation.rotationUpdated (r)
    UserOrientation.updateDeviceAngle (r)
    UserOrientation.getInferredOrientation (_, o)
    Camera.setAdditionalRotation (o)
</code></pre>
<p>When the accelerometer reports a change in the rotation of the device, a new inferred orientation is obtained from the new angle, and the additional rotation in the camera is updated accordingly.</p>
<h2 id="connecting-back-to-the-api">Connecting back to the API<a hidden class="anchor" aria-hidden="true" href="#connecting-back-to-the-api">#</a></h2>
<p>Now looking back at the API code sample we can understand it more readily:</p>
<p><img loading="lazy" src="orientation-changed.png" alt=""  />
</p>
<p>The firing of the <em>onOrientationChanged</em> listener corresponds to the <em>DeviceRotation.rotationUpdated</em> action that fires the sync; the <em>when</em> statement that assigns rotation based on orientation is an implementation of the <em>UserOrientation.getInferredOrientation</em> action; and the setting of <em>imageCapture.targetRotation</em> is the <em>Camera.setAdditionalRotation</em> action.</p>
<p>In the alternative code sample,</p>
<p><img loading="lazy" src="display-listener.png" alt=""  />
</p>
<p>my guess is that the following is going on. There is a separate and fuller implementation of the <em>UserOrientation</em> concept that rotates the display for apps in general (and is appropriately sync’d with the other concepts as above). The camera can use the  orientation reported directly by this module, obtaining the <em>rotation</em> field of the <em>display</em> object. This presumably is affected by whether the orientation lock is on, and perhaps also by whether the app in question is using this feature.</p>
<h2 id="summary">Summary<a hidden class="anchor" aria-hidden="true" href="#summary">#</a></h2>
<p>In summary, we’ve resolved our initial questions as follows:</p>
<ul>
<li>The word “orientation” seems to be used in two distinct ways: for the device rotation, and for the inferred orientation.</li>
<li>The “sensor rotation” is just the default rotation associated with captured images, and the “image rotation” is their total associated rotation.</li>
<li>Because the phone already includes a module that infers orientation and rotates the <em>display</em> accordingly, display orientation and device orientation are distinct, and a programmer might be able to obtain the display orientation directly, bypassing the need to make the inference in application code.</li>
</ul>
<p>What are the benefits of the conceptual view here? I’d argue that they are:</p>
<ul>
<li>A clean <strong>separation of concerns</strong>, into (a) obtaining the device rotation from the accelerometer; (b) storing rotations with images; and (c) inferring orientations from rotations.</li>
<li>A <strong>purpose</strong> and simple <strong>scenarios</strong> (OPs) for the concepts, in particular the <em>UserOrientation</em> concept which the API docs didn’t make clear is heuristic.</li>
</ul>
<p>It remains to be seen how this kind of concept design can best be worked into API documentation. It’s clearly not ideal to have a textual notation for concepts that is different from the typical interface specs of APIs. One possibility is to represent concepts diagrammatically.</p>
<p>Another challenge involves mapping the concepts to the sometimes complex object-oriented patterns that are used in APIs, which are often more like frameworks (with application code written as subclassing extensions) than service interfaces.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>

  </footer>
</article>
    </main>
    
<footer class="footer">


    <span>&copy; 2023 <a href="https://essenceofsoftware.com">Daniel Jackson</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
